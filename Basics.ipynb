{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "828c06a3",
   "metadata": {},
   "source": [
    "# NLP Basic Concept\n",
    "## Lexical Analysis\n",
    "### 1. Tokenization\n",
    "Breaks down raw text into smaller, meaningful units called tokens (words, subwords, or characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbe47f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/adam/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f95de70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: ['NLTK tokenizing is a crucial step in NLP.', 'It is widely used.']\n",
      "Words: ['NLTK', 'tokenizing', 'is', 'a', 'crucial', 'step', 'in', 'NLP', '.', 'It', 'is', 'widely', 'used', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"NLTK tokenizing is a crucial step in NLP. It is widely used.\"\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "words = word_tokenize(text)\n",
    "\n",
    "print(\"Sentences:\", sentences)\n",
    "print(\"Words:\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af613e01",
   "metadata": {},
   "source": [
    "### 2. Case folding\n",
    "Converts all characters in a text to a single case (usually lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18fb879b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nltk tokenizing is a crucial step in nlp. it is widely used.\n"
     ]
    }
   ],
   "source": [
    "print(text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b99d81b",
   "metadata": {},
   "source": [
    "### 3. Punctuation Removal\n",
    "Only retain the important word by removing punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b60e842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Are you there \n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "teks = \"Hello!!! Are you there??? :)\"\n",
    "print(''.join([char for char in teks if char not in string.punctuation]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd77e99",
   "metadata": {},
   "source": [
    "### 4. Stop word removal\n",
    "Filters out common, less meaningful words (like \"the,\" \"is,\" \"a\") to reduce noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac5584a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/adam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3e03258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example', 'stop', 'word', 'removal', '.']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(\"This is an example of stop word removal.\")\n",
    "print([word for word in words if word.lower() not in stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9160173c",
   "metadata": {},
   "source": [
    "### 5. Abbreviations Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b4b78e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbc141cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr Smith is an M.D from US\n"
     ]
    }
   ],
   "source": [
    "text = \"Dr Smith is an M.D from U.S.\"\n",
    "abbrev_cleaned = re.sub(r'\\b(Dr|Mr|Ms|M\\.D|U\\.S)\\.', lambda x : x.group(0).replace('.', ''), text)\n",
    "print(abbrev_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8828f036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prof John lives in the UK and works at MIT\n"
     ]
    }
   ],
   "source": [
    "sent = \"Prof John lives in the U.K. and works at M.I.T.\"\n",
    "fixed_sent = re.sub(r'\\b([A-Z])\\.', r'\\1', sent)\n",
    "print(fixed_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89db08f7",
   "metadata": {},
   "source": [
    "### 6. Stemming\n",
    "Chops off word endings (suffixes/prefixes) to reduce words to their common \"stem\" or root form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41d9bc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28d6f2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'run', 'runner']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "words = ['running', 'runs', 'runner']\n",
    "print([stemmer.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5ca755",
   "metadata": {},
   "source": [
    "### 7. Part-of-speech tagging\n",
    "Assigning grammatical categories (like noun, verb, adjective) to each word in a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b8159a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/adam/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5a40109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(\"The quick brown fox jumps over the lazy dog.\")\n",
    "print(nltk.pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7bc72b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google       | PROPN  | nsubj\n",
      "is           | AUX    | aux\n",
      "looking      | VERB   | ROOT\n",
      "at           | ADP    | prep\n",
      "buying       | VERB   | pcomp\n",
      "a            | DET    | det\n",
      "startup      | NOUN   | dobj\n",
      "in           | ADP    | prep\n",
      "London       | PROPN  | pobj\n",
      "for          | ADP    | prep\n",
      "$            | SYM    | quantmod\n",
      "1            | NUM    | compound\n",
      "billion      | NUM    | pobj\n",
      ".            | PUNCT  | punct\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"Google is looking at buying a startup in London for $1 billion.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print word and its POS tag\n",
    "for token in doc:\n",
    "    print(f\"{token.text:12} | {token.pos_:6} | {token.dep_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1238386",
   "metadata": {},
   "source": [
    "### 8. Word Sense Disambiguition\n",
    "Identifying the correct meaning (sense) of a polysemous word (a word with multiple meanings) in a specific context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec7e2231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/adam/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dae45f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('depository_financial_institution.n.01') a financial institution that accepts deposits and channels the money into lending activities\n",
      "Synset('depository_financial_institution.n.01')\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I went to the bank to deposit money.\"\n",
    "synset1 = lesk(word_tokenize(sentence), \"bank\")\n",
    "print(synset1, synset1.definition())\n",
    "print(synset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "191e5ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('book.n.02') physical objects consisting of a number of pages bound together\n",
      "Synset('book.n.02')\n"
     ]
    }
   ],
   "source": [
    "sentence2 = \"The book is full of notes.\"\n",
    "synset2 = lesk(word_tokenize(sentence2), \"book\")\n",
    "print(synset2, synset2.definition())\n",
    "print(synset2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be74dd02",
   "metadata": {},
   "source": [
    "### 9. Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7315eba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf46656a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "こんにちは お元気ですか？\n",
      "こんにちは\n",
      "龙\n"
     ]
    }
   ],
   "source": [
    "# example 1\n",
    "translated1 = GoogleTranslator(source='auto', target='ja').translate(\"Hello, how are you?\")\n",
    "print(translated1)\n",
    "\n",
    "# example 2\n",
    "translated2 = GoogleTranslator(source='auto', target='ja').translate(\"Hi\")\n",
    "print(translated2)\n",
    "\n",
    "user_text = input(\"Enter text to translate: \")\n",
    "translated_text = GoogleTranslator(source='auto', target='zh-CN').translate(user_text)\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a81e220",
   "metadata": {},
   "source": [
    "### 10. Name Entity Recognition\n",
    "Finding and classifying real-world entities in text, like people, organizations, locations, dates, and monetary values, into predefined categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1a35277",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     /home/adam/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/adam/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package words to /home/adam/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('words')\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e85fbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Barack/NNP)\n",
      "  (PERSON Obama/NNP)\n",
      "  was/VBD\n",
      "  born/VBN\n",
      "  in/IN\n",
      "  (GPE Hawaii/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Barack Obama was born in Hawaii.\"\n",
    "tree = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ec79225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities found:\n",
      "Google          | ORG\n",
      "London          | GPE\n",
      "$1 billion      | MONEY\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is looking at buying a startup in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    London\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $1 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using the same 'doc' from the POS example\n",
    "print(\"Entities found:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:15} | {ent.label_}\")\n",
    "\n",
    "# Pro-tip: Visualizing it inside a notebook\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105622a9",
   "metadata": {},
   "source": [
    "### 11. Vectorization: Bag of Words (CountVectorizer)\n",
    "It creates a matrix where each row is a document and each column is a word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "149ff652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   about  analysis  and  core  data  for  great  is  learning  love  nlp  of  \\\n",
      "0      0         1    0     0     0    1      1   1         0     0    1   0   \n",
      "1      0         1    0     1     1    0      0   1         0     0    0   1   \n",
      "2      1         0    1     0     1    0      0   0         1     1    1   0   \n",
      "\n",
      "   part  science  text  \n",
      "0     0        0     1  \n",
      "1     1        1     1  \n",
      "2     0        1     0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "corpus = [\n",
    "    \"NLP is great for text analysis.\",\n",
    "    \"Text analysis is a core part of data science.\",\n",
    "    \"I love learning about NLP and data science.\"\n",
    "]\n",
    "\n",
    "# Initialize and transform\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert to a readable DataFrame\n",
    "df_bow = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(df_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd97b284",
   "metadata": {},
   "source": [
    "### 12. Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "TF-IDF is smarter than Bag of Words. It rewards rare, meaningful words (like \"NLP\") and penalizes common words (like \"is\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9f65654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      about  analysis       and      core      data       for     great  \\\n",
      "0  0.000000  0.366180  0.000000  0.000000  0.000000  0.481482  0.481482   \n",
      "1  0.000000  0.313316  0.000000  0.411973  0.313316  0.000000  0.000000   \n",
      "2  0.417567  0.000000  0.417567  0.000000  0.317570  0.000000  0.000000   \n",
      "\n",
      "         is  learning      love      nlp        of      part   science  \\\n",
      "0  0.366180  0.000000  0.000000  0.36618  0.000000  0.000000  0.000000   \n",
      "1  0.313316  0.000000  0.000000  0.00000  0.411973  0.411973  0.313316   \n",
      "2  0.000000  0.417567  0.417567  0.31757  0.000000  0.000000  0.317570   \n",
      "\n",
      "       text  \n",
      "0  0.366180  \n",
      "1  0.313316  \n",
      "2  0.000000  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vec.fit_transform(corpus)\n",
    "\n",
    "# Show the weights (numbers closer to 1 are more \"important\" words)\n",
    "df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vec.get_feature_names_out())\n",
    "print(df_tfidf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e54db5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adam_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
